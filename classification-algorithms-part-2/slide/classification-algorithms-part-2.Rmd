---
title: "分类算法（下）"
subtitle: "Classification Algorithms - Part 2"
author: "范叶亮 | Leo Van"
date: ""
output:
  xaringan::moon_reader:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    css:
      - "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
      - "https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"
      - "css/style.css"
    includes:
      after_body: "includes/after_body.html"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: content

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

<div class="content-page">
  <p class="content-page-title">目录</p>
  <li class="content-page-list">Bagging</li>
  <li class="content-page-list">Boosting</li>
  <li class="content-page-list">Stacking</li>
</div>

---
class:

# 集成学习

传统的机器学习算法（例如：决策树，人工神经网络，支持向量机，朴素贝叶斯等）的目标都是寻找一个最优的分类器尽可能的将训练数据分开。集成学习（Ensemble Learning）算法的基本思想就是通过将多个分类器组合，从而实现一个预测效果更好的集成分类器。集成算法可以说从一方面验证了中国的一句老话：三个臭皮匠，赛过诸葛亮。Thomas G. Dietterich <sup>[1, 2]</sup> 指出了集成算法在统计，计算和表示上的有效原因：

**统计上的原因**：一个学习算法可以理解为在一个假设空间 $\mathcal{H}$ 中选找到一个最好的假设。但是，当训练样本的数据量小到不够用来精确的学习到目标假设时，学习算法可以找到很多满足训练样本的分类器。所以，学习算法选择任何一个分类器都会面临一定错误分类的风险，因此将多个假设集成起来可以降低选择错误分类器的风险。

.footnote[
[1] Dietterich, Thomas G. "Ensemble methods in machine learning." _International workshop on multiple classifier systems_. Springer, Berlin, Heidelberg, 2000.  
[2] Dietterich, Thomas G. "Ensemble learning." _The handbook of brain theory and neural networks_ 2 (2002): 110-125.
]

---
class:

# 集成学习

**计算上的原因**：很多学习算法在进行最优化搜索时很有可能陷入局部最优的错误中，因此对于学习算法而言很难得到一个全局最优的假设。事实上人工神经网络和决策树已经被证实为是一个 NP 问题 <sup>[1, 2]</sup>。集成算法可以从多个起始点进行局部搜索，从而分散陷入局部最优的风险。

**表示上的原因**：在多数应用场景中，假设空间 $\mathcal{H}$ 中的任意一个假设都无法表示（或近似表示）真正的分类函数 $f$。因此，对于不同的假设条件，通过加权的形式可以扩大假设空间，从而学习算法可以在一个无法表示或近似表示真正分类函数 $f$ 的假设空间中找到一个逼近函数 $f$ 的近似值。

.footnote[
[1] Laurent, Hyafil, and Ronald L. Rivest. "Constructing optimal binary decision trees is NP-complete." _Information processing letters_ 5.1 (1976): 15-17.  
[2] Blum, Avrim L., and Ronald L. Rivest. "Training a 3-node neural network is NP-complete." _Neural Networks_ 5.1 (1992): 117-127.
]

---
class: section, center, middle

# Bagging

---
class:

# Bagging

.pull-left[
Bagging 是由 Breiman 于 1996 年提出 <sup>[1]</sup>，基本思想如下：

1. 每次采用有放回的抽样从训练集中取出 $n$ 个训练样本组成新训练集。
2. 利用新的训练集，训练得到 $M$ 个子模型 $\{h_1, h_2, ..., h_M\}$。
3. 对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值。
]

.pull-right[
<pre class="convert-pseudocode" caption="Bagging 算法">
\REQUIRE 学习算法 $L$，子模型个数 $M$，训练数据集 $T$
\ENSURE Bagging 算法 $h_f\left(x\right)$
\PROCEDURE{Bagging}{$L, M, T$}
    \FOR{$m$ = $1$ to $M$}
        \STATE $T_m \gets$ bootstrap sample from training set $T$
        \STATE $h_m \gets L\left(T_m\right)$
    \ENDFOR
    \STATE $h_f\left(x\right) \gets \mathbb{argmax}_{y \in Y} \sum_{m}^{M} h_i\left(x\right)$
    \RETURN $h_f\left(x\right)$
\ENDPROCEDURE
</pre>
]

.footnote[
[1] Breiman, Leo. "Bagging predictors." _Machine learning_ 24.2 (1996): 123-140.
]

---
class:

# Bagging

假设对于一个包含 $M$ 个样本的数据集 $T$，利用自助采样，则一个样本始终不被采用的概率是 $\left(1 - \dfrac{1}{M}\right)^M$，取极限有：

$$\lim_{x \to \infty} \left(1 - \dfrac{1}{M}\right)^M = \dfrac{1}{e} \approx 0.368$$

即每个学习器仅用到了训练集中 $63.2\%$ 的数据集，剩余的 $36.8\%$ 的训练集样本可以用作验证集对于学习器的泛化能力进行包外估计（out-of-bag estimate）。

---
class:

# 随机森林

随机森林（Random Forests）<sup>[1]</sup> 是一种以决策树为基学习器的 Bagging 集成学习算法。随机森林模型的构建过程如下：

- 数据采样

作为一种 Bagging 的集成算法，随机森林同样采用有放回的采样，对于总体训练集 $T$，抽样一个子集 $T_{sub}$ 最为训练样本集。除此之外，假设训练集的特征个数为 $d$，每次仅选择 $k\left(k < d\right)$ 个构建决策树。因此，随机森林处理能够做到样本扰动外，还添加了特征扰动，对于特征的选择个数，推荐值 $k = \log_2 d$。

- 树的构建

每次根据采样得到的数据和特征，构建一棵决策树。在构建决策树的过程中，会让决策树生长完全而不进行剪枝。构建出的若干棵决策树则组成了最终的随机森林。

.footnote[
[1] Breiman, Leo. "Random forests." _Machine learning_ 45.1 (2001): 5-32.
]

---
class:

# 随机森林

随机森林在众多分类算法中表现十分出众 <sup>[1]</sup>，其主要的优点包括：

1. 由于随机森林引入了样本扰动和特征扰动，从而很大程度上提高了模型的泛化能力，尽可能地避免了过拟合现象的出现。
2. 随机森林可以处理高维数据，无需进行特征选择，在训练过程中可以得出不同特征对模型的重要性程度。
3. 随机森林的每个弱分类器采用决策树，方法简单且容易实现。同时每个弱分类器之间没有相互依赖关系，整个算法易并行化。

.footnote[
[1] Fernández-Delgado, Manuel, et al. "Do we need hundreds of classifiers to solve real world classification problems?." _The journal of machine learning research_ 15.1 (2014): 3133-3181.
]

---
class:

# 随机森林

.pull-left[
```{r}
set.seed(1)
library(mlr3)
library(mlr3learners)

task_wine <- mlr_tasks$get('wine')
train_set_ids <- sample(
  task_wine$nrow, 0.8 * task_wine$nrow)
test_set_ids <- setdiff(
  seq_len(task_wine$nrow), train_set_ids)

learner <- lrn('classif.ranger')
learner$train(task_wine, row_ids = train_set_ids)

pred <- learner$predict(
  task_wine, row_ids = test_set_ids)
```
]

.pull-right[
```{r}
pred$confusion
pred$score(msr('classif.acc'))
```
]

---
class: section, center, middle

# Boosting

---
class:

# Boosting

Boosting 是一种提升算法，可以将弱的学习算法提升（boost）为强的学习算法。其基本思路如下：

1. 利用初始训练样本集训练得到一个基学习器。
2. 提高被基学习器误分的样本的权重，使得那些被错误分类的样本在下一轮训练中可以得到更大的关注，利用调整后的样本训练得到下一个基学习器。
3. 重复上述步骤，直到得出 $M$ 个学习器。
4. 对于分类问题，采用有权重的投票方式；对于回归问题，采用加权平均得到预测值。

---
class:

# Adaboost

Adaboost <sup>[1]</sup> 是 Boosting 算法中最具代表性的一个。原始的 Adaboost 算法用于解决二分类问题，因此对于一个训练集

$$
T = \{\left(x_1, y_1\right), \left(x_2, y_2\right), ..., \left(x_n, y_n\right)\}
$$

其中 $x_i \in \mathcal{X} \subseteq \mathbb{R}^n$，
$y_i \in \mathcal{Y} = \{-1, +1\}$，首先初始化训练集的权重

$$
\begin{split}
D_1 =& \left(w_{11}, w_{12}, ..., w_{1n}\right) \\
 w_{1i} =& \dfrac{1}{n}, i = 1, 2, ..., n
\end{split}
$$

根据每一轮训练集的权重 $D_m$，对训练集数据进行抽样得到 $T_m$，再根据 $T_m$ 可以得到每一轮的基学习器 $h_m$。

.footnote[
[1] Freund, Yoav, and Robert E. Schapire. "A desicion-theoretic generalization of on-line learning and an application to boosting." _European conference on computational learning theory_. Springer, Berlin, Heidelberg, 1995.
]

---
class:

# Adaboost

通过计算可以得出基学习器 $h_m$ 的误差为 $\epsilon_m$，根据基学习器的误差计算得出该基学习器在最终学习器中的系数

$$
\alpha_m = \dfrac{1}{2} \ln \dfrac{1 - \epsilon_m}{\epsilon_m}
$$

更新训练集的权重

$$
\begin{split}
D_{m+1} =& \left(w_{m+1, 1}, w_{m+1, 2}, ..., w_{m+1, n}\right) \\
w_{m+1, i} =& \dfrac{w_{m, i}}{Z_m} \exp \left(-\alpha_m y_i h_m\left(x_i\right)\right) \\
Z_m =& \sum_{i = 1}^{n} w_{m, i} \exp \left(-\alpha_m y_i h_m \left(x_i\right)\right)
\end{split}
$$

其中 $Z_m$ 为规范化因子，从而保证 $D_{m+1}$ 为一个概率分布。

---
class:

.pull-left[
# Adaboost

最终根据构建的 $M$ 个基学习器得到最终的学习器：

$$h_f\left(x\right) = \text{sign}\left(\sum_{m=1}^{M} \alpha_i h_m\left(x\right)\right)$$

AdaBoost 算法过程如由所示：
]

.pull-right[
<br />
<br />

<pre class="convert-pseudocode" caption="AdaBoost 算法">
\REQUIRE 学习算法 $L$，子模型个数 $M$，训练数据集 $T$
\ENSURE AdaBoost 算法 $h_f\left(x\right)$
\PROCEDURE{AdaBoost}{$L, M, T$}
    \STATE $D_1\left(x\right) \gets 1 / n$
    \FOR{$m$ = $1$ to $M$}
        \STATE $T_{sub} \gets $ sample from training set $T$ with weights
        \STATE $h_m \gets L\left(T_{sub}\right)$
        \STATE $\epsilon_m \gets Error\left(h_m\right)$
        \IF{$\epsilon_m > 0.5$}
            \STATE \textbf{break}
        \ENDIF
        \STATE $\alpha_m \gets \dfrac{1}{2} \ln \dfrac{1 - \epsilon_m}{\epsilon_m}$
        \STATE $D_{m+1} \gets \dfrac{D_m \exp \left(-\alpha_m y h_m\left(x\right)\right)}{Z_m}$
    \ENDFOR
    \STATE $h_f\left(x\right) \gets \mathbf{sign}\left(\sum_{m=1}^{M} \alpha_i h_m\left(x\right)\right)$
    \RETURN $h_f\left(x\right)$
\ENDPROCEDURE
</pre>
]

---
class:

# GBM

GBM（Gradient Boosting Machine）是另一种基于 Boosting 思想的集成算法，GBM 还有很多其他的叫法，例如：GBDT，GBRT，MART等等。GBM 算法由 3 个主要概念构成：Gradient Boosting（GB），Regression Decision Tree（DT 或 RT）和 Shrinkage。

从 GBM 的众多别名中可以看出，GBM 中使用的决策树并非我们最常用的分类树，而是回归树。分类树主要用于处理响应变量为因子型的数据，例如天气（可以为晴，阴或下雨等）。回归树主要用于处理响应变量为数值型的数据，例如商品的价格。当然回归树也可以用于二分类问题，对于回归树预测出的数值结果，通过设置一个阈值即可以将数值型的预测结果映射到二分类问题标签上，即 $\mathcal{Y} = \{-1, +1\}$。

对于 Gradient Boosting 而言，首先，Boosting 并不是 Adaboost 中的 Boost 的概念，也不是 Random Forest 中的冲抽样。在 Adaboost 中，Boost 是指在生成每个新的基学习器，跟根据上一轮基学习器分类对错对训练集设置不同的权重，使得在上一轮中分类错误的样本在生成新的基学习器时更被重视。GBM 中在应用 Boost 概念时，每一轮所使用的数据集没有经过重抽样，也没有更新样本的权重，而是每一轮选择了不用的回归的目标值，即上一轮计算得出的残差（Residual）。其次，Gradient 是指在新一轮中在残差减少的梯度（Gradient）上建立新的基学习器。

---
class:

# GBM

下面通过一个年龄预测的示例介绍 GBM 的工作流程。存在 4 个人 $P = \{p_1, p_2, p_3, p_4\}$，他们对应的年龄为 $14, 16, 24, 26$。其中 $p_1, p_2$ 分别是高一和高三学生，
$p_3, p_4$ 分别是应届毕业生和工作两年的员工。利用决策树模型进行训练可以得到如图所示的结果：

.center[
![](images/gbm-workflow-original-dession-tree.svg)
]

---
class:

# GBM

利用 GBM 训练得到模型，由于数据量少，在此限定每个基学习器中的叶子节点最多为 2 个，即树的深度最大为 1 层。训练得到的结果如图所示：

.pull-left[
.center[
![](images/gbm-workflow-gbm-step-1.svg)
]
]

.pull-right[
.center[
![](images/gbm-workflow-gbm-step-2.svg)
]
]

在训练第一棵树过程中，利用年龄作为预测值，根据计算可得由于 $p_1, p_2$ 年龄相近，
$p_3, p_4$ 年龄相近被划分为两组。通过计算两组中真实年龄和预测的年龄的差值，可以得到第一棵树的残差 $R = \{-1, 1, -1, 1\}$。因此在训练第二棵树的过程中，利用第一棵树的残差作为预测值，最终所有人的年龄均正确被预测，即最终所有的残差均为 $0$。

---
class:

# GBM

.pull-left[
则对于训练集中的 4 个人利用训练得到额 GBM 模型预测的结果如下：

- $p_1$：14岁高一学生。购物较少，经常问学长问题，预测年龄 $Age = 15 - 1 = 14$。
- $p_2$：16岁高三学生。购物较少，经常被学弟问问题，预测年龄 $Age = 15 + 1 = 16$。
- $p_3$：24岁应届毕业生。购物较多，经常问师兄问题，预测年龄 $Age = 25 - 1 = 24$。
- $p_4$：26岁2年工作经验员工。购物较多，经常被师兄问问题，预测年龄 $Age = 25 + 1 = 26$。
]

.pull-right[
<pre class="convert-pseudocode" caption="GBM 算法">
\REQUIRE 子模型个数 $M$，训练数据集 $T$
\ENSURE GBM 算法 $h_f\left(x\right)$
\PROCEDURE{GBM}{$M, T$}
    \STATE $F_1\left(x\right) \gets \sum_{i = 1}^{N} y_i / N$
    \FOR {$m$ = $1$ to $M$}
        \STATE $r_m \gets y - F_m \left(x\right)$
        \STATE $T_m \gets \left(x, r_m\right)$
        \STATE $h_m \gets RegressionTree \left(T_m\right)$
        \STATE $\alpha_m \gets \dfrac{\sum_{i = 1}^{N} r_{im} h_m \left(x_i\right)}{\sum_{i = 1}^{N} h_m \left(x_i\right)^2}$
        \STATE $F_m \left(x\right) = F_{m-1} \left(x\right) + \alpha_m h_m \left(x\right)$
    \ENDFOR
    \STATE $h_f\left(x\right) =  F_M \left(x\right)$
    \RETURN $h_f\left(x\right)$
\ENDPROCEDURE
</pre>
]

---
class:

# GBM

在 GBM 中也应用到了 Shrinkage 的思想，其基本思想可以理解为在每一轮利用残差学习得到的回归树仅学习到了一部分知识，即无法完全信任一棵树的结果。因此，Shrinkage 思想认为在新的一轮学习中，不利用全部残差训练模型，而只利用其中一部分，即：

$$r_m = y - s F_m \left(x\right), 0 \leq s \leq 1$$

注意，这里的 Shrinkage 和学习算法中 Gradient 的步长是两个不相关的概念。Shrinkage 设置小一些可以避免发生过拟合现象；而 Gradient 中的步长如果设置太小则会陷入局部最优，如果设置过大又容易结果不收敛。

---
class:

# GBM

.pull-left[
```{r message=F, warning=F}
set.seed(1)
library(mlr3)
library(mlr3extralearners)
library(gbm)

task_wine <- mlr_tasks$get('wine')
train_set_ids <- sample(
  task_wine$nrow, 0.8 * task_wine$nrow)
test_set_ids <- setdiff(
  seq_len(task_wine$nrow), train_set_ids)

learner <- lrn(
  'classif.gbm', distribution = 'multinomial')
learner$train(task_wine, row_ids = train_set_ids)

pred <- learner$predict(
  task_wine, row_ids = test_set_ids)
```
]

.pull-right[
```{r}
pred$confusion
pred$score(msr('classif.acc'))
```
]

---
class:

# XGBoost

.pull-left[
```{r}
set.seed(1)
library(mlr3)
library(mlr3learners)
library(xgboost)

task_wine <- mlr_tasks$get('wine')
train_set_ids <- sample(
  task_wine$nrow, 0.8 * task_wine$nrow)
test_set_ids <- setdiff(
  seq_len(task_wine$nrow), train_set_ids)

learner <- lrn('classif.xgboost')
learner$train(task_wine, row_ids = train_set_ids)

pred <- learner$predict(
  task_wine, row_ids = test_set_ids)
```
]

.pull-right[
```{r}
pred$confusion
pred$score(msr('classif.acc'))
```
]

---
class: section, center, middle

# Stacking

---
class:

# Stacking

Stacking 本身是一种集成学习方法，同时也是一种模型组合策略，我们首先介绍一些相对简单的模型组合策略：**平均法**和**投票法**。

对于 数值型的输出 $h_i \left(\mathbf{x}\right) \in \mathbb{R}$，

- 简单平均法 (Simple Averaging)

$$
H \left(\mathbf{x}\right) = \dfrac{1}{M} \sum_{i=1}^{M}{h_i \left(\mathbf{x}\right)}
$$

- 加权平均法 (Weighted Averaging)

$$
H \left(\mathbf{x}\right) = \sum_{i=1}^{M}{w_i h_i \left(\mathbf{x}\right)}
$$

其中，
$w_i$ 为学习器 $h_i$ 的权重，且 $w_i \geq 0, \sum_{i=1}^{T}{w_i} = 1$。

---
class:

# Stacking

对于 分类型的任务，学习器 $h_i$ 从类别集合 $\left\{c_1, c_2, \dotsc, c_N\right\}$ 中预测一个标签。我们将 $h_i$ 在样本 $\mathbf{x}$ 上的预测输出表示为一个 $N$ 维向量 $\left(h_i^1 \left(\mathbf{x}\right); h_i^2 \left(\mathbf{x}\right); \dotsc, h_i^N \left(\mathbf{x}\right)\right)$，其中 $h_i^j \left(\mathbf{x}\right)$ 为 $h_i$ 在类型标签 $c_j$ 上的输出。

- 绝对多数投票法 (Majority Voting)

$$H \left(\mathbf{x}\right) = \left\{
\begin{array}{c}
c_j, & \displaystyle\sum_{i=1}^{M}{h_i^j \left(\mathbf{x}\right) > 0.5 \displaystyle\sum_{k=1}^{N}{\displaystyle\sum_{i=1}^{M}{h_i^k \left(\mathbf{x}\right)}}} \\
\text{拒绝}, & \text{其他情况}
\end{array}\right.$$

即如果一个类型的标记得票数过半，则预测为该类型，否则拒绝预测。

- 相对多数投票法 (Plurality Voting)

$$H \left(\mathbf{x}\right) = c_{\arg\max_j \sum_{i=1}^{M}{h_i^j \left(\mathbf{x}\right)}}$$

即预测为得票数最多的类型，如果同时有多个类型获得相同最高票数，则从中随机选取一个。

---
class:

# Stacking

- 加权投票法 （Weighted Voting)

$$H \left(\mathbf{x}\right) = c_{\arg\max_j \sum_{i=1}^{M}{w_i h_i^j \left(\mathbf{x}\right)}}$$

其中，
$w_i$ 为学习器 $h_i$ 的权重，且 $w_i \geq 0, \sum_{i=1}^{M}{w_i} = 1$。

绝对多数投票提供了“拒绝预测”，这为可靠性要求较高的学习任务提供了一个很好的机制，但如果学习任务要求必须有预测结果时则只能选择相对多数投票法和加权投票法。在实际任务中，不同类型的学习器可能产生不同类型的 $h_i^j \left(\boldsymbol{x}\right)$ 值，常见的有：

- 类标记，
$h_i^j \left(\mathbf{x}\right) \in \left\{0, 1\right\}$，若 $h_i$ 将样本 $\mathbf{x}$ 预测为类型 $c_j$ 则取值为 1，否则取值为 0。使用类型标记的投票称之为 “硬投票” (Hard Voting)。
- 类概率，
$h_i^j \left(\mathbf{x}\right) \in \left[0, 1\right]$，相当于对后验概率 $P \left(c_j \ | \ \mathbf{x}\right)$ 的一个估计。使用类型概率的投票称之为 “软投票” (Soft Voting)。

---
class:

# Stacking

.pull-left[
Stacking <sup>[1, 2]</sup> 方法又称为 Stacked Generalization，是一种基于分层模型组合的集成算法。Stacking 算法的基本思想如下：

1. 利用初级学习算法对原始数据集进行学习，同时生成一个新的数据集。
2. 根据从初级学习算法生成的新数据集，利用次级学习算法学习并得到最终输出。

对于初级学习器，可以是相同类型也可以是不同类型，同时在生成新的数据集的过程中，其相应变量仍为原始数据集中的相应变量。Stacking 算法流程如图所示：
]

.pull-right[
<br/><br/><br/>
![](images/stacking.png)
]

.footnote[
[1] Wolpert, David H. "Stacked generalization." _Neural networks_ 5.2 (1992): 241-259.  
[2] Breiman, Leo. "Stacked regressions." _Machine learning_ 24.1 (1996): 49-64.
]

---
class:

# Stacking

.pull-left[
<pre class="convert-pseudocode" caption="Stacking 算法">
\REQUIRE \\
    初级学习算法 $L = \{L_1, L_2, ... L_M\}$ \\
    次级学习算法 $L'$ \\
    训练数据集 $T = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), ..., (\mathbf{x}_N, y_N)\}$
\ENSURE Stacking 算法 $h_f\left(x\right)$
\PROCEDURE{Stacking}{$L, L', T$}
\FOR{$m$ = $1$ to $M$}
  \STATE $h_t \gets L_m \left(T\right)$
\ENDFOR
\STATE $T' \gets \varnothing$
\STATE \COMMENT{接下文}
\ENDPROCEDURE
</pre>
]

.pull-right[
<pre class="convert-pseudocode">
\PROCEDURE{Stacking}{$L, L', T$}
\STATE \COMMENT{接上文}
\FOR{$i$ = $1$ to $N$}
  \FOR{$m$ = $1$ to $M$}
    \STATE $z_{im} \gets h_m(\mathbf{x}_i)$
  \ENDFOR
  \STATE $T' \gets T' \cup \left(\left(z_{i1}, z_{i2}, ..., z_{iM}\right), y_i\right)$
\ENDFOR
\STATE $h' \gets L' \left(T'\right)$
\STATE $h_f\left(\mathbf{x}\right) \gets h' \left(h_1\left(\mathbf{x}\right), h_2\left(\mathbf{x}\right), ..., h_M\left(\mathbf{x}\right)\right)$
\RETURN $h_f\left(\mathbf{x}\right)$
\ENDPROCEDURE
</pre>
]

---
class:

# Stacking

.pull-left[
次级学习器的训练集是由初级学习器产生的，如果直接利用初级学习器的训练集生成次级学习器的训练集，则过拟合风险会比较大 <sup>[1]</sup>。因此，一般利用在训练初始学习器中未使用过的样本来产生次级学习器的训练样本。以 $k$ 折交叉检验为例：初始的训练集 $T$ 被随机划分为 $k$ 个大小相近的集合 $T_1, T_2, ..., T_k$。令 $T_j$ 和 $\bar{T_j}$ 表示第 $j$ 折的测试集和训练集。则对于 $M$ 个初级学习算法，其学习器 $h_m^{\left(j\right)}$ 是根据训练集 $\bar{T_j}$ 生成的，对于测试集 $T_j$ 中的每个样本 $x_i$，得到 $z_{im} = h_m^{\left(j\right)} \left(x_i\right)$。则根据 $x_i$ 所产生的次级学习器的训练样本为 $\left(\left(z_{i1}, z_{i2}, ..., z_{iM}\right), y_i\right)$。最终利用 $M$ 个初级学习器产生的训练集 $T' = \{\left(z_i, y_i\right)\}_{i=1}^N$ 训练次级学习器。
]

.pull-right[
```{r, echo=F, out.width='80%', fig.align='center'}
knitr::include_graphics('images/clfs-decision-regions.png')
```
]

.footnote[
[1] 周志华, _机器学习_. 清华大学出版社, 2016.
]

---
class:

# Stacking

.pull-left[
```{r}
set.seed(1)
library(tidyverse)
library(mlr3)
library(mlr3pipelines)

dst <- lrn('classif.rpart', predict_type = 'prob')
svm <- lrn('classif.svm', predict_type = 'prob')
lr <- lrn('classif.log_reg', predict_type = 'prob')

stack <- gunion(list(
  PipeOpLearnerCV$new(dst$clone(), id = 'decision_tree_cv'),
  PipeOpLearnerCV$new(svm$clone(), id = 'svm_cv'))) %>>%
  PipeOpFeatureUnion$new(2, id = 'feature_union') %>>%
  PipeOpLearner$new(lr$clone(), id = 'logistic_regression')

stack$plot()
```
]

.pull-right[
```{r include=F}
png('generated/stacking-graph.png', width = 960, height = 800, res = 200)
par(
  mar = c(0, 0, 0, 0),
  oma = c(0, 0, 0, 0))
stack$plot()
dev.off()
```

![](generated/stacking-graph.png)
]

---
class:

# Stacking

.pull-left[
```{r message=F, warning=F}
stack_lrn <- GraphLearner$new(stack)
stack_lrn$train(task_wine, row_ids = train_set_ids)
```
]

.pull-right[
```{r}
pred <- learner$predict(
  task_wine, row_ids = test_set_ids)
pred$confusion
pred$score(msr('classif.acc'))
```
]

---
class: thanks, center, middle

# Thanks

![CC BY-NC-SA 4.0](assets/by-nc-sa.svg)

本作品采用 [**CC BY-NC-SA 4.0**](https://creativecommons.org/licenses/by-nc-sa/4.0/) 进行许可

Copyright © [**范叶亮 | Leo Van**](https://leovan.me), All Rights Reserved.